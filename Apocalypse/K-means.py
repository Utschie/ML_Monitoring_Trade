#把每一帧拉直成向量再标准化后用聚类方法把无限种帧的情况转化成有限个类(用1，2，3表示)，因为doc2vec需要有限个数的词汇表，这样就把每场比赛都用有限多个类代号组成的长串来表示
#如果不搞词汇表的话那就是相当于直接拉直，每一种情况都是一个unique词汇
#然后用gensim的Doc2vec（或者GloVe或者sentence2vec）得到每一场比赛的向量+xgboost或随机森林或fasttext或深度森林模型对序列进行操作————20210517
#本程序暂时是使用K均值聚类，对训练集进行聚类，然后输出一个包含所有训练集比赛的符合gensim的doc2vec输入规则的文件————20210517
#之后可以尝试子空间分割聚类，毕竟拉直后的帧的向量有至少4000多个指标，维度很高而且很稀疏————20210517
#可以把数值的数据用谱聚类给聚类成有限个类名，然后文字型的当做单独的词汇也写入单个样本的这个Taged_document里,然后再进入doc2vec————20210517
#用普通kmeans聚类很有可能内存会爆掉，所以暂时使用sklearn的minibatchkmeans
#在训练及59367个文件里，共有121554754次，即1.2亿次转移,近算上有数据的数据行的话，有22.5亿行————20210524
#这里隐藏了一个可能的问题，就是赔率的数据是偏态分布，它不可能小于1,但是上不封顶，不知道对标准化会不会有影响————20210525
#赔率的分布更接近卡方分布，及左边有界，然后隆起，然后迅速下降至0,此时用平方根法可以把分布转化成正态分布（不过这里可能设计到一个正负号的问题）————20210525
#随机抽100场比赛的所有行，标准化后进行主成分分析，方差解释度为[0.4339, 0.3027, 0.1411 , 0.0585, 0.0310,0.0147 , 0.0102, 0.0051, 0.0023]
#如果随机抽10000场比赛，每场比赛只提供最后两帧的话，标准化后进行主成分分析，方差解释度为[0.368, 0.2548, 0.1777, 0.0715, 0.0537,0.0277, 0.0220, 0.0179, 0.0059]
#如果所有列取对数，然后用PCA包自带的标准化库标准化后做主成分分析，方差解释度为[0.44,0.21,0.18,0.076,0.04...]
#如果只有赔率列取对数，则方差解释度为[0.42971035, 0.20809988, 0.16917282, 0.08865169, 0.05207994,0.04530344, 0.00458814, 0.00159367, 0.00079466]
#赔率列取了倒数方差解释度也差不多，取了平方根也差不多，就是基本上都得取前4个才能保存足够总方差解释度————20210526
#如果先用数字的均值和标准差把每一帧标准化，然后补0,拉直成6010维的向量，然后再进行pca的话，那么方差解释度为（未完成）
#如果先把每一帧拉直，然后通过拉直后的均值和标准差（包含0求出的那个均值和方差），那么方差解释度为（未完成）
#如果前面两个都不太行，实在不行那就给每个公司都分配一个权重，然后取加权平均值，然后再取前4-5个主成分
#如果实在都不行的话，那就只能各行求简单平均了
#使用pca或者svd降维，应该对全样本做一个pca或者svd而不是每个都做一次，然后保存得到的特征向量矩阵，每次通过这个固定的矩阵进行数据变换————20210527
#或者使用因子分析————20210527
#pca和svd降维本质上都是用原始数据乘以奇异值分解的右矩阵把每行缩短，但是没法把行数变少，除非拉直————20210527
#在使用拉直后的均值和标准差的情况下,如果仅选择25场比赛，那么前50个指标就可以解释90%+的方差，但是如果所有比赛都在一起59000多场比赛按批次IncrementalPCA的话，前50个指标就只能解释68%的方差————20210606
#如果使用纯数值行的均值和标准差，然后无数值的部分用0补全，再拉直后做IncrementalPCA，选取25000场每场随机抽最多两帧，进行pca后的50个主成分的方差解释率为69%————20210627
#用上述方法即用num_mean，随机选取25000场每场随机抽最多两帧，100个主成分的pca的总方差解释率为76%，200个主成分为83%，400个主成分为89%,800个主成分刚好95%————20210627
#如果使用flatten_mean,那么50个主成分解释率70%，100个主成分为79%，200个主成分为88%，400个主成分为95.7%————20210627
#还有一种方法，就是只让有数字行做pca，也就是features只有10个，这个时候做pca要至少保留6个主成分才能达到90%以上的解释率，所以这个道路行不通————20210627
#用flatten_num_mean的方法，然后是纯粹pca（而不是incrementalPCA），400个主成分大概只有88%，而使用flatten_pca（纯PCA），400个主成分解释率可以到达96%————20210628
#如果去掉凯利指数和概率，只保留返还率和赔率，然后赔率全部去倒数，在不进行标准化的情况下主成分分析，第一个因子可以有90%的解释率，但是不中心化用PCA是不行的————20210628
#所以最终还是决定采用flatten_pca的方法取前400个主成分,即flatten_pca_400.m文件————20210628
#filelist[2]和filelist[5]由同一家公司开出初赔，初赔值不一样，经过pca转换后的400维向量也不一样，而且没有一个值相同
#也就是说，初赔的不同反映在每个主成分里，而第一主成分主要反映的是开出赔率的公司数量，数量不同会影响很大
#而公司不同则不会太影响第一主成分，哪怕
#不过不同的那部分相比与第一主成分那部分太小了
#另外，对于同一场比赛，对于开赔公司数量相同的帧，他们的pca向量是一模一样的
#那可能还不如直接用平均赔率做单帧的向量呢妈的————20210629
#而如果用平均赔率做单帧向量的话，那基本上就是数据预处理沿用类似textRNN_half_ubuntu2的特征选择方法，只是模型用的不同————20210629
#用了20个特征分类成50000类的kmeans后，每场比赛确实被分成了10个以上的类，但是还是有个问题，就是类变化主要出现在前面没出多少个赔率的时候，到后期1000多次的赔率变化它还是分不出来的————20210703
#而之所以分不出来，其中一个原因是用平均值和标准差计算特征的时候，由于后期的变动很小，导致连续两帧的特征是一样的，所以自然分不出来————20210703
#同时也有因为类别太少导致的，尽管值不同，但是类归在了一起,由于特征粒度太粗的而导致的分不出来可能得有倒着数1000帧，由于分类粒度导致的分布出来再加2000帧————20210703
#不过如果变盘程度挺大的也能分出来，有的1500帧的比赛甚至比3000多帧的比赛所拥有的类还要多————20210703
#所以其实可以就把每一次类别变换当作一次变大盘，于是每一场比赛就从几千次变动变成了十几次变动，起到了缩短序列的作用————20210703
#doc2vec来自与word2vec，其参数量大致等于词向量维度×词汇表规模，如果用90维的词向量，则总参数量为90×50000=4500000个参数，假如窗口长度为5,那平均每个句子大概是10个样本，总样本量则为500000,显然是有点太少的————20210703
#由于用20个特征，那么其实1000多帧到3000多帧的比赛里，每场比赛大概200上下个不同值，那实际上可以就仅仅取用每场比赛的不同值统一做一个kmeans，这样的话那其实总共59000多场比赛充其量也就1千万左右的不同值，直接对这个做一个普通的kmeans就可与————20210703
#用均值和标准差做特征，那么所有的1.2亿帧里共有9329757个不同的值————20210704
#即便只采用unique的值做聚类，900万×20的矩阵进行直接kmeans还是太大了，所以还是采用了minibatchkmeans，结果其实还是跟之前差不多就是一场比赛差不多20个左右的类的长度————20210705
#而结果跟之前也没太大差别，就是到最后就基本上稳定住是一个类
#所以现在按照分50000类来算，相当与词汇表的量有50000,那么拿60000场比赛，平均每场比赛不过20次的长度来训练词向量的话显然感觉不是特别的够，所以尝试看一下如果只分成5000类会不会好一些
#如果只分成5000类，那么词向量只需要70维，参数量就变成了70×5000=350000,而分成5000类之后，平均每场比赛的长度多少降低了些，不过没那么多，减少窗口或许也有差不多相当与500000个样本，但是也不是很多。
#所以现在就有两套方案，一套是用每场比赛unique值化的方法，不聚类把比赛长度缩短，然后放进rnn或者cnn之类的，再一套就是用这些比赛来训练词向量，然后扔进xgboost